# LinkedIn Scraper & API Service

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.13+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.0+-green.svg)](https://fastapi.tiangolo.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Supported-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

</div>

Este proyecto automatiza la extracciÃ³n de ofertas de trabajo de LinkedIn, las almacena en una base de datos PostgreSQL y expone un **API REST** para operaciones CRUD. Incluye ademÃ¡s un script de automatizaciÃ³n para backups y cargas incrementales.

---

## ğŸ“‹ Objetivo

1. **Scraping** de LinkedIn con Selenium + BeautifulSoup (por renderizado JS)  
2. **Almacenamiento** en PostgreSQL (local o en la nube)  
3. **API REST** en FastAPI para crear, leer y eliminar ofertas  
4. **AutomatizaciÃ³n** de backups, limpieza de respaldos antiguos y cargas incrementales

---

## ğŸ› ï¸ Pre-requisitos

- Python â‰¥ 3.13  
- Docker & Docker-Compose (opcional, para levantar PostgreSQL local)  
- ChromeDriver compatible con tu versiÃ³n de Chrome  
- Credenciales de LinkedIn (usuario/contraseÃ±a)

---

## âš™ï¸ ConfiguraciÃ³n

### 1. Clonar y configurar el entorno

```bash
# Clonar el repositorio
git clone https://github.com/DillanDevs/linkedin_scraping.git
cd linkedin_scraping

# Crear variables de entorno
touch .env
```

### 2. Editar el archivo .env

```ini
LINKEDIN_USER=tu_usuario
LINKEDIN_PASS=tu_contraseÃ±a
DATABASE_URL=postgresql+psycopg2://backend:backend@localhost:5432/authdb
```

### 3. Levantar base de datos local con Docker-Compose

AsegÃºrate de tener este `docker-compose.yaml` en la raÃ­z:

```yaml
version: "3.8"
services:
  db:
    image: postgres:14
    environment:
      POSTGRES_USER: backend
      POSTGRES_PASSWORD: backend
      POSTGRES_DB: authdb
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data
volumes:
  db_data:
```

```bash
docker-compose up -d
```

### 4. Instalar dependencias

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

## â–¶ï¸ Uso

### 1. Ejecutar Scraping

Extrae datos y los guarda en `dataset_linkedin.csv` y en la base de datos:

```bash
python -m scraping.pipeline
```

### 2. Arrancar API REST

```bash
uvicorn backend.main:app \
  --reload \
  --host 0.0.0.0 \
  --port 8000
```

DocumentaciÃ³n interactiva disponible en: http://localhost:8000/docs

### 3. AutomatizaciÃ³n (opcional)

```bash
python -m backend.scripts.automation schedule
```

**Sub-comandos disponibles:**
- `scrape` â†’ ejecuta scraping inmediato
- `backup` â†’ realiza volcado completo con pg_dump
- `cleanup` â†’ elimina respaldos mÃ¡s antiguos que RETENTION_DAYS

---

## ğŸ›ï¸ Diagrama de arquitectura

![alt text](diagrama.png)

---

## ğŸ“‚ Estructura del proyecto

```
linkedin_scraping/
â”œâ”€â”€ .env                      # Variables de entorno
â”œâ”€â”€ docker-compose.yaml       # ConfiguraciÃ³n de PostgreSQL local
â”œâ”€â”€ requirements.txt          # Dependencias del proyecto
â”‚
â”œâ”€â”€ scraping/                 # MÃ³dulo de extracciÃ³n de datos
â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n del scraper
â”‚   â”œâ”€â”€ driver.py             # GestiÃ³n de Selenium
â”‚   â”œâ”€â”€ pipeline.py           # Pipeline completo de scraping
â”‚   â”œâ”€â”€ scraper.py            # LÃ³gica de scraping con BS4
â”‚   â”œâ”€â”€ to_db.py              # Carga en base de datos
â”‚   â””â”€â”€ utils.py              # Utilidades
â”‚
â””â”€â”€ backend/                  # Servicio REST API
    â”œâ”€â”€ api/                  # DefiniciÃ³n de rutas
    â”œâ”€â”€ config/               # ConfiguraciÃ³n
    â”œâ”€â”€ models/               # Modelos ORM
    â”œâ”€â”€ repository/           # Capa de acceso a datos
    â”œâ”€â”€ schemas/              # Esquemas Pydantic
    â”œâ”€â”€ services/             # LÃ³gica de negocio
    â”œâ”€â”€ scripts/              # Scripts de automatizaciÃ³n
    â”œâ”€â”€ utils/                # Utilidades y logging
    â””â”€â”€ main.py               # Punto de entrada
```

---

## âœ… Recomendaciones

### Orden de ejecuciÃ³n:
1. Primero el pipeline de scraping (genera CSV + carga DB)
2. Luego el API para consultar los datos

### Entorno local vs nube:
- **Local**: Con Docker-Compose 
  ```
  DATABASE_URL=postgresql+psycopg2://backend:backend@localhost:5432/authdb
  ```
- **En nube**: Ajusta `DATABASE_URL` a tu RDS, Cloud SQL u otro servicio PostgreSQL

---

## ğŸ“Š Endpoints disponibles

| MÃ©todo | Ruta | DescripciÃ³n |
|--------|------|-------------|
| GET | `/jobs/` | Listar todas las ofertas de trabajo |
| GET | `/jobs/{id}` | Obtener oferta por ID |
| POST | `/jobs/` | Crear o actualizar mÃºltiples ofertas |
| DELETE | `/jobs/{id}` | Eliminar oferta por ID |

---

## ğŸ“œ Licencia

Este proyecto estÃ¡ licenciado bajo la [Licencia MIT](LICENSE).

---

<div align="center">
Desarrollado por DillanDevs Â© 2025
</div>

